# Cloud Classification

## Overview
This project is a cloud classification pipeline that classifies clouds into one of two types based on features generated from cloud images. The goal is to make the model reproducible across different environments and accessible for everyone. The model was initially developed in a Jupyter notebook and has been transitioned into a series of scripts for production.

The steps followed in the pipeline are:

1. Load configuration from local file. 
2. Acquire and clean/normalize the data. 
3. Generate features from the data. 
4. Train a simple supervised model. 
5. Score the model and capture performance metrics. 
6. Save key artifacts (metadata, config, training data, model, metrics) to disk. 
7. Upload the artifacts to an AWS S3 bucket. 

All these steps are composed in a single pipeline.py script which makes use of functions defined in the project's Python package.

## Directory Structure

The project root directory contains three main files and four directories:  
1. `pipeline.py`: This is the main script that runs the entire cloud classification pipeline.
2. `requirements.txt`: This file lists the Python dependencies required to run the pipeline.
3. `test-requirements.txt`: This file lists the Python dependencies required to run the unit tests.

**Directories:**
1. `src`: This directory contains eight Python modules that are used in the pipeline. Each module corresponds to a different step in the pipeline:
- `acquire_data.py`: This module is responsible for acquiring the data.
- `create_dataset.py`: This module creates a structured dataset from the raw data.
- `generate_features.py`: This module enriches the dataset with features for model training.
- `analysis.py`: This module generates statistics and visualizations to summarize the data.
- `train_model.py`: This module trains the model on the training dataset.
- `score_model.py`: This module scores the model on the test dataset.
- `evaluate_performance.py`: This module evaluates the performance of the model.
- `aws_utils.py`: This module uploads the artifacts to an AWS S3 bucket.
2. `config`: This directory contains the configuration files in YAML format which are used to configure the pipeline.
3. `tests`: This directory contains `test_generate_features.py` that is used to do unit tests for the `generate_features.py`.
4. `dockerfiles`: This directory contains Dockerfiles for running the pipeline and unit tests. `dockerfile_pipeline` is used for running the pipeline, while `dockerfile_unittest` is used for running the unit tests.

## Setup

Follow these steps to set up the project:

**1. Clone the repository:**
```
git clone https://github.com/MSIA/2023-423--scn3674--hw2.git
```

**2. Navigate to the project directory:**
```
cd 2023-423--scn3674--hw2
```

**3. Create a virtual environment (Optional but recommended):**
```
python3 -m venv venv
source venv/bin/activate
```

**4. Install the requirements:**
```
pip install -r requirements.txt
```

**5. Fetch the data:**

The data fetching process is integrated into the pipeline and will be automatically executed when running `pipeline.py`.


## Running the Pipeline
**Way 1**

- To run the pipeline, use the following command:
```
python pipeline.py
```

- The artifacts generated by the pipeline will be saved in a timestamped directory under the runs directory, and optionally uploaded to an AWS S3 bucket if specified in the configuration.

**Way 2**
- First, build the Docker image for the pipeline:  
```
docker build -t pipeline_image -f dockerfiles/dockerfile_pipeline .
```

- After building the image, you can run the pipeline using the following command:  
```
docker run -v ~/.aws:/root/.aws -e AWS_PROFILE=default pipeline_image
```

- Before running the Docker commands, ensure that the AWS_PROFILE environment variable is set to "default". You can do this by adding the following line to your shell initialization script (e.g., ~/.bashrc or ~/.zshrc):  
```
export AWS_PROFILE=default
```
After adding the line, make sure to source the script:

For bash:

```
source ~/.bashrc
```

For zsh:

```
source ~/.zshrc
```

## Running the Unit Tests

**Way 1**
- First, install the Python dependencies required to run the unit tests by running the following command:  

```
pip install -r test-requirements.txt
```

- Navigate to the tests directory and run the following command:

```
pytest
```

pytest will automatically discover and run the test files in the `tests` directory.  

**Way 2** 

The second way is to run in a docker container

- First, build the Docker image for the unit tests:

```
docker build -t unittest_image -f dockerfiles/dockerfile_unittest .
```

- After building the image, you can run the unit tests using the following command:

```
docker run unittest_image
```
